{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of the line search algorithm from the scipy package, applied to complex-valued variables.\n",
    "# Not working for the ptychography case right now.\n",
    "\n",
    "To adapt the line search algorithm, I changed the Wolfe conditions from the original code to that described in the paper (Equation 3.10):\n",
    "\n",
    "https://epubs.siam.org/doi/10.1137/110832124\n",
    "\n",
    "Changes made:\n",
    "\n",
    "* Change the return value of the derphi functions in line_search_wolfe2 from   \n",
    "`np.dot(gval[0], pk)`  \n",
    "to   \n",
    "`np.real(np.dot(np.conj(gval[0]), pk))`.\n",
    "\n",
    "* Similarly, change  \n",
    "`derphi0 = np.dot(gfk, pk)`  \n",
    "to  \n",
    "`derphi0 = np.real(np.dot(np.conj(gfk), pk))`\n",
    "\n",
    "* Finally, change the return value of the `extra_condition` function supplied by the calling functions in optimize.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "from scipy._lib.six import xrange\n",
    "\n",
    "__all__ = ['LineSearchWarning',  'line_search_wolfe2',\n",
    "           'scalar_search_wolfe2',\n",
    "           'line_search_armijo']\n",
    "\n",
    "class LineSearchWarning(RuntimeWarning):\n",
    "    pass\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Pure-Python Wolfe line and scalar searches\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None,\n",
    "                       old_old_fval=None, args=(), c1=1e-4, c2=0.9, amax=None,\n",
    "                       extra_condition=None, maxiter=10):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function.\n",
    "    myfprime : callable f'(x,*args)\n",
    "        Objective function gradient.\n",
    "    xk : ndarray\n",
    "        Starting point.\n",
    "    pk : ndarray\n",
    "        Search direction.\n",
    "    gfk : ndarray, optional\n",
    "        Gradient value for x=xk (xk being the current parameter\n",
    "        estimate). Will be recomputed if omitted.\n",
    "    old_fval : float, optional\n",
    "        Function value for x=xk. Will be recomputed if omitted.\n",
    "    old_old_fval : float, optional\n",
    "        Function value for the point preceding x=xk\n",
    "    args : tuple, optional\n",
    "        Additional arguments passed to objective function.\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "    extra_condition : callable, optional\n",
    "        A callable of the form ``extra_condition(alpha, x, f, g)``\n",
    "        returning a boolean. Arguments are the proposed step ``alpha``\n",
    "        and the corresponding ``x``, ``f`` and ``g`` values. The line search \n",
    "        accepts the value of ``alpha`` only if this \n",
    "        callable returns ``True``. If the callable returns ``False`` \n",
    "        for the step length, the algorithm will continue with \n",
    "        new iterates. The callable is only called for iterates \n",
    "        satisfying the strong Wolfe conditions.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float or None\n",
    "        Alpha for which ``x_new = x0 + alpha * pk``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    fc : int\n",
    "        Number of function evaluations made.\n",
    "    gc : int\n",
    "        Number of gradient evaluations made.\n",
    "    new_fval : float or None\n",
    "        New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    old_fval : float\n",
    "        Old function value ``f(x0)``.\n",
    "    new_slope : float or None\n",
    "        The local slope along the search direction at the\n",
    "        new value ``<myfprime(x_new), pk>``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "    \"\"\"\n",
    "    fc = [0]\n",
    "    gc = [0]\n",
    "    gval = [None]\n",
    "    gval_alpha = [None]\n",
    "\n",
    "    def phi(alpha):\n",
    "        fc[0] += 1\n",
    "        return f(xk + alpha * pk, *args)\n",
    "\n",
    "    if isinstance(myfprime, tuple):\n",
    "        def derphi(alpha):\n",
    "            fc[0] += len(xk) + 1\n",
    "            eps = myfprime[1]\n",
    "            fprime = myfprime[0]\n",
    "            newargs = (f, eps) + args\n",
    "            gval[0] = fprime(xk + alpha * pk, *newargs)  # store for later use\n",
    "            gval_alpha[0] = alpha\n",
    "            return np.real(np.dot(np.conj(gval[0]), pk))\n",
    "    else:\n",
    "        fprime = myfprime\n",
    "\n",
    "        def derphi(alpha):\n",
    "            gc[0] += 1\n",
    "            gval[0] = fprime(xk + alpha * pk, *args)  # store for later use\n",
    "            gval_alpha[0] = alpha\n",
    "            return np.real(np.dot(np.conj(gval[0]), pk))\n",
    "\n",
    "    if gfk is None:\n",
    "        gfk = fprime(xk, *args)\n",
    "    derphi0 = np.real(np.dot(np.conj(gfk), pk))\n",
    "\n",
    "    if extra_condition is not None:\n",
    "        # Add the current gradient as argument, to avoid needless\n",
    "        # re-evaluation\n",
    "        def extra_condition2(alpha, phi):\n",
    "            if gval_alpha[0] != alpha:\n",
    "                derphi(alpha)\n",
    "            x = xk + alpha * pk\n",
    "            return extra_condition(alpha, x, phi, gval[0])\n",
    "    else:\n",
    "        extra_condition2 = None\n",
    "\n",
    "    alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(\n",
    "            phi, derphi, old_fval, old_old_fval, derphi0, c1, c2, amax,\n",
    "            extra_condition2, maxiter=maxiter)\n",
    "\n",
    "    if derphi_star is None:\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "    else:\n",
    "        # derphi_star is a number (derphi) -- so use the most recently\n",
    "        # calculated gradient used in computing it derphi = gfk*pk\n",
    "        # this is the gradient at the next step no need to compute it\n",
    "        # again in the outer loop.\n",
    "        derphi_star = gval[0]\n",
    "\n",
    "    return alpha_star, fc[0], gc[0], phi_star, old_fval, derphi_star\n",
    "\n",
    "\n",
    "def scalar_search_wolfe2(phi, derphi=None, phi0=None,\n",
    "                         old_phi0=None, derphi0=None,\n",
    "                         c1=1e-4, c2=0.9, amax=None,\n",
    "                         extra_condition=None, maxiter=10):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : callable f(x)\n",
    "        Objective scalar function.\n",
    "    derphi : callable f'(x), optional\n",
    "        Objective function derivative (can be None)\n",
    "    phi0 : float, optional\n",
    "        Value of phi at s=0\n",
    "    old_phi0 : float, optional\n",
    "        Value of phi at previous point\n",
    "    derphi0 : float, optional\n",
    "        Value of derphi at s=0\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "    extra_condition : callable, optional\n",
    "        A callable of the form ``extra_condition(alpha, phi_value)``\n",
    "        returning a boolean. The line search accepts the value\n",
    "        of ``alpha`` only if this callable returns ``True``.\n",
    "        If the callable returns ``False`` for the step length,\n",
    "        the algorithm will continue with new iterates.\n",
    "        The callable is only called for iterates satisfying\n",
    "        the strong Wolfe conditions.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform\n",
    "    Returns\n",
    "    -------\n",
    "    alpha_star : float or None\n",
    "        Best alpha, or None if the line search algorithm did not converge.\n",
    "    phi_star : float\n",
    "        phi at alpha_star\n",
    "    phi0 : float\n",
    "        phi at 0\n",
    "    derphi_star : float or None\n",
    "        derphi at alpha_star, or None if the line search algorithm\n",
    "        did not converge.\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "    \"\"\"\n",
    "\n",
    "    if phi0 is None:\n",
    "        phi0 = phi(0.)\n",
    "\n",
    "    if derphi0 is None and derphi is not None:\n",
    "        derphi0 = derphi(0.)\n",
    "\n",
    "    alpha0 = 0\n",
    "    if old_phi0 is not None and derphi0 != 0:\n",
    "        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n",
    "    else:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if alpha1 < 0:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    phi_a1 = phi(alpha1)\n",
    "    #derphi_a1 = derphi(alpha1)  evaluated below\n",
    "\n",
    "    phi_a0 = phi0\n",
    "    derphi_a0 = derphi0\n",
    "\n",
    "    if extra_condition is None:\n",
    "        extra_condition = lambda alpha, phi: True\n",
    "\n",
    "    for i in xrange(maxiter):\n",
    "        if alpha1 == 0 or (amax is not None and alpha0 == amax):\n",
    "            # alpha1 == 0: This shouldn't happen. Perhaps the increment has\n",
    "            # slipped below machine precision?\n",
    "            alpha_star = None\n",
    "            phi_star = phi0\n",
    "            phi0 = old_phi0\n",
    "            derphi_star = None\n",
    "\n",
    "            if alpha1 == 0:\n",
    "                msg = 'Rounding errors prevent the line search from converging'\n",
    "            else:\n",
    "                msg = \"The line search algorithm could not find a solution \" + \\\n",
    "                      \"less than or equal to amax: %s\" % amax\n",
    "            \n",
    "            warn(msg, LineSearchWarning)\n",
    "            break\n",
    "\n",
    "        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or \\\n",
    "           ((phi_a1 >= phi_a0) and (i > 1)):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha0, alpha1, phi_a0,\n",
    "                              phi_a1, derphi_a0, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2, extra_condition)\n",
    "            break\n",
    "\n",
    "        derphi_a1 = derphi(alpha1)\n",
    "        if (abs(derphi_a1) <= -c2*derphi0):\n",
    "            if extra_condition(alpha1, phi_a1):\n",
    "                alpha_star = alpha1\n",
    "                phi_star = phi_a1\n",
    "                derphi_star = derphi_a1\n",
    "                break\n",
    "\n",
    "        if (derphi_a1 >= 0):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha1, alpha0, phi_a1,\n",
    "                              phi_a0, derphi_a1, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2, extra_condition)\n",
    "            break\n",
    "\n",
    "        alpha2 = 2 * alpha1  # increase by factor of two on each iteration\n",
    "        if amax is not None:\n",
    "            alpha2 = min(alpha2, amax)\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi(alpha1)\n",
    "        derphi_a0 = derphi_a1\n",
    "\n",
    "    else:\n",
    "        # stopping test maxiter reached\n",
    "        alpha_star = alpha1\n",
    "        phi_star = phi_a1\n",
    "        derphi_star = None\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "\n",
    "    return alpha_star, phi_star, phi0, derphi_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a cubic polynomial that goes through the\n",
    "    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.\n",
    "    If no minimizer can be found return None\n",
    "    \"\"\"\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _quadmin(a, fa, fpa, b, fb):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a quadratic polynomial that goes through\n",
    "    the points (a,fa), (b,fb) with derivative at a of fpa,\n",
    "    \"\"\"\n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,\n",
    "          phi, derphi, phi0, derphi0, c1, c2, extra_condition):\n",
    "    \"\"\"\n",
    "    Part of the optimization algorithm in `scalar_search_wolfe2`.\n",
    "    \"\"\"\n",
    "    \n",
    "    maxiter = 10\n",
    "    i = 0\n",
    "    delta1 = 0.2  # cubic interpolant check\n",
    "    delta2 = 0.1  # quadratic interpolant check\n",
    "    phi_rec = phi0\n",
    "    a_rec = 0\n",
    "    while True:\n",
    "        # interpolate to find a trial step length between a_lo and\n",
    "        # a_hi Need to choose interpolation here.  Use cubic\n",
    "        # interpolation and then if the result is within delta *\n",
    "        # dalpha or outside of the interval bounded by a_lo or a_hi\n",
    "        # then use quadratic interpolation, if the result is still too\n",
    "        # close, then use bisection\n",
    "\n",
    "        dalpha = a_hi - a_lo\n",
    "        if dalpha < 0:\n",
    "            a, b = a_hi, a_lo\n",
    "        else:\n",
    "            a, b = a_lo, a_hi\n",
    "\n",
    "        # minimizer of cubic interpolant\n",
    "        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n",
    "        #\n",
    "        # if the result is too close to the end points (or out of the\n",
    "        # interval) then use quadratic interpolation with phi_lo,\n",
    "        # derphi_lo and phi_hi if the result is still too close to the\n",
    "        # end points (or out of the interval) then use bisection\n",
    "\n",
    "        \n",
    "        \n",
    "        if (i > 0):\n",
    "            cchk = delta1 * dalpha\n",
    "            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,\n",
    "                            a_rec, phi_rec)\n",
    "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
    "            qchk = delta2 * dalpha\n",
    "            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                a_j = a_lo + 0.5*dalpha\n",
    "        \n",
    "        # Check new value of a_j\n",
    "\n",
    "        phi_aj = phi(a_j)\n",
    "        if (phi_aj > phi0 + c1 * a_j * derphi0) or (phi_aj >= phi_lo):\n",
    "            phi_rec = phi_hi\n",
    "            a_rec = a_hi\n",
    "            a_hi = a_j\n",
    "            phi_hi = phi_aj\n",
    "        else:\n",
    "            derphi_aj = derphi(a_j)\n",
    "            if derphi_aj <= -c2*derphi0 and extra_condition(a_j, phi_aj):\n",
    "                a_star = a_j\n",
    "                val_star = phi_aj\n",
    "                valprime_star = derphi_aj\n",
    "                break\n",
    "            if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                phi_rec = phi_hi\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_lo\n",
    "                phi_hi = phi_lo\n",
    "            else:\n",
    "                phi_rec = phi_lo\n",
    "                a_rec = a_lo\n",
    "            a_lo = a_j\n",
    "            phi_lo = phi_aj\n",
    "            derphi_lo = derphi_aj\n",
    "        i += 1\n",
    "        \n",
    "        if (i > maxiter):\n",
    "            # Failed to find a conforming step size\n",
    "            a_star = None\n",
    "            val_star = None\n",
    "            valprime_star = None\n",
    "            break\n",
    "    return a_star, val_star, valprime_star\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Armijo line and scalar searches\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def line_search_armijo(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):\n",
    "    \"\"\"Minimize over alpha, the function ``f(xk+alpha pk)``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function to be minimized.\n",
    "    xk : array_like\n",
    "        Current point.\n",
    "    pk : array_like\n",
    "        Search direction.\n",
    "    gfk : array_like\n",
    "        Gradient of `f` at point `xk`.\n",
    "    old_fval : float\n",
    "        Value of `f` at point `xk`.\n",
    "    args : tuple, optional\n",
    "        Optional arguments.\n",
    "    c1 : float, optional\n",
    "        Value to control stopping criterion.\n",
    "    alpha0 : scalar, optional\n",
    "        Value of `alpha` at start of the optimization.\n",
    "    Returns\n",
    "    -------\n",
    "    alpha\n",
    "    f_count\n",
    "    f_val_at_alpha\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the interpolation algorithm (Armijo backtracking) as suggested by\n",
    "    Wright and Nocedal in 'Numerical Optimization', 1999, pg. 56-57\n",
    "    \"\"\"\n",
    "    xk = np.atleast_1d(xk)\n",
    "    fc = [0]\n",
    "\n",
    "    def phi(alpha1):\n",
    "        fc[0] += 1\n",
    "        return f(xk + alpha1*pk, *args)\n",
    "\n",
    "    if old_fval is None:\n",
    "        phi0 = phi(0.)\n",
    "    else:\n",
    "        phi0 = old_fval  # compute f(xk) -- done in past loop\n",
    "\n",
    "    derphi0 = np.real(np.dot(gfk, pk))\n",
    "    alpha, phi1 = scalar_search_armijo(phi, phi0, derphi0, c1=c1,\n",
    "                                       alpha0=alpha0)\n",
    "    return alpha, fc[0], phi1\n",
    "\n",
    "\n",
    "def line_search_BFGS(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):\n",
    "    \"\"\"\n",
    "    Compatibility wrapper for `line_search_armijo`\n",
    "    \"\"\"\n",
    "    r = line_search_armijo(f, xk, pk, gfk, old_fval, args=args, c1=c1,\n",
    "                           alpha0=alpha0)\n",
    "    return r[0], r[1], 0, r[2]\n",
    "\n",
    "\n",
    "def scalar_search_armijo(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
    "    \"\"\"Minimize over alpha, the function ``phi(alpha)``.\n",
    "    Uses the interpolation algorithm (Armijo backtracking) as suggested by\n",
    "    Wright and Nocedal in 'Numerical Optimization', 1999, pg. 56-57\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "    Returns\n",
    "    -------\n",
    "    alpha\n",
    "    phi1\n",
    "    \"\"\"\n",
    "    phi_a0 = phi(alpha0)\n",
    "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
    "        return alpha0, phi_a0\n",
    "\n",
    "    # Otherwise compute the minimizer of a quadratic interpolant:\n",
    "\n",
    "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
    "    phi_a1 = phi(alpha1)\n",
    "\n",
    "    if (phi_a1 <= phi0 + c1*alpha1*derphi0):\n",
    "        return alpha1, phi_a1\n",
    "\n",
    "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
    "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
    "    # assume that the value of alpha is not too small and satisfies the second\n",
    "    # condition.\n",
    "\n",
    "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
    "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
    "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
    "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
    "        a = a / factor\n",
    "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
    "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
    "        b = b / factor\n",
    "\n",
    "        alpha2 = (-b + np.sqrt(abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
    "        phi_a2 = phi(alpha2)\n",
    "\n",
    "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
    "            return alpha2, phi_a2\n",
    "\n",
    "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
    "            alpha2 = alpha1 / 2.0\n",
    "\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi_a2\n",
    "\n",
    "    # Failed to find a suitable step length\n",
    "    return None, phi_a1\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Non-monotone line search for DF-SANE\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def _nonmonotone_line_search_cruz(f, x_k, d, prev_fs, eta,\n",
    "                                  gamma=1e-4, tau_min=0.1, tau_max=0.5):\n",
    "    \"\"\"\n",
    "    Nonmonotone backtracking line search as described in [1]_\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function returning a tuple ``(f, F)`` where ``f`` is the value\n",
    "        of a merit function and ``F`` the residual.\n",
    "    x_k : ndarray\n",
    "        Initial position\n",
    "    d : ndarray\n",
    "        Search direction\n",
    "    prev_fs : float\n",
    "        List of previous merit function values. Should have ``len(prev_fs) <= M``\n",
    "        where ``M`` is the nonmonotonicity window parameter.\n",
    "    eta : float\n",
    "        Allowed merit function increase, see [1]_\n",
    "    gamma, tau_min, tau_max : float, optional\n",
    "        Search parameters, see [1]_\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        Step length\n",
    "    xp : ndarray\n",
    "        Next position\n",
    "    fp : float\n",
    "        Merit function value at next position\n",
    "    Fp : ndarray\n",
    "        Residual at next position\n",
    "    References\n",
    "    ----------\n",
    "    [1] \"Spectral residual method without gradient information for solving\n",
    "        large-scale nonlinear systems of equations.\" W. La Cruz,\n",
    "        J.M. Martinez, M. Raydan. Math. Comp. **75**, 1429 (2006).\n",
    "    \"\"\"\n",
    "    f_k = prev_fs[-1]\n",
    "    f_bar = max(prev_fs)\n",
    "\n",
    "    alpha_p = 1\n",
    "    alpha_m = 1\n",
    "    alpha = 1\n",
    "\n",
    "    while True:\n",
    "        xp = x_k + alpha_p * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= f_bar + eta - gamma * alpha_p**2 * f_k:\n",
    "            alpha = alpha_p\n",
    "            break\n",
    "\n",
    "        alpha_tp = alpha_p**2 * f_k / (fp + (2*alpha_p - 1)*f_k)\n",
    "\n",
    "        xp = x_k - alpha_m * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= f_bar + eta - gamma * alpha_m**2 * f_k:\n",
    "            alpha = -alpha_m\n",
    "            break\n",
    "\n",
    "        alpha_tm = alpha_m**2 * f_k / (fp + (2*alpha_m - 1)*f_k)\n",
    "\n",
    "        alpha_p = np.clip(alpha_tp, tau_min * alpha_p, tau_max * alpha_p)\n",
    "        alpha_m = np.clip(alpha_tm, tau_min * alpha_m, tau_max * alpha_m)\n",
    "\n",
    "    return alpha, xp, fp, Fp\n",
    "\n",
    "\n",
    "def _nonmonotone_line_search_cheng(f, x_k, d, f_k, C, Q, eta,\n",
    "                                   gamma=1e-4, tau_min=0.1, tau_max=0.5,\n",
    "                                   nu=0.85):\n",
    "    \"\"\"\n",
    "    Nonmonotone line search from [1]\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Function returning a tuple ``(f, F)`` where ``f`` is the value\n",
    "        of a merit function and ``F`` the residual.\n",
    "    x_k : ndarray\n",
    "        Initial position\n",
    "    d : ndarray\n",
    "        Search direction\n",
    "    f_k : float\n",
    "        Initial merit function value\n",
    "    C, Q : float\n",
    "        Control parameters. On the first iteration, give values\n",
    "        Q=1.0, C=f_k\n",
    "    eta : float\n",
    "        Allowed merit function increase, see [1]_\n",
    "    nu, gamma, tau_min, tau_max : float, optional\n",
    "        Search parameters, see [1]_\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        Step length\n",
    "    xp : ndarray\n",
    "        Next position\n",
    "    fp : float\n",
    "        Merit function value at next position\n",
    "    Fp : ndarray\n",
    "        Residual at next position\n",
    "    C : float\n",
    "        New value for the control parameter C\n",
    "    Q : float\n",
    "        New value for the control parameter Q\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] W. Cheng & D.-H. Li, ''A derivative-free nonmonotone line\n",
    "           search and its application to the spectral residual\n",
    "           method'', IMA J. Numer. Anal. 29, 814 (2009).\n",
    "    \"\"\"\n",
    "    alpha_p = 1\n",
    "    alpha_m = 1\n",
    "    alpha = 1\n",
    "\n",
    "    while True:\n",
    "        xp = x_k + alpha_p * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= C + eta - gamma * alpha_p**2 * f_k:\n",
    "            alpha = alpha_p\n",
    "            break\n",
    "\n",
    "        alpha_tp = alpha_p**2 * f_k / (fp + (2*alpha_p - 1)*f_k)\n",
    "\n",
    "        xp = x_k - alpha_m * d\n",
    "        fp, Fp = f(xp)\n",
    "\n",
    "        if fp <= C + eta - gamma * alpha_m**2 * f_k:\n",
    "            alpha = -alpha_m\n",
    "            break\n",
    "\n",
    "        alpha_tm = alpha_m**2 * f_k / (fp + (2*alpha_m - 1)*f_k)\n",
    "\n",
    "        alpha_p = np.clip(alpha_tp, tau_min * alpha_p, tau_max * alpha_p)\n",
    "        alpha_m = np.clip(alpha_tm, tau_min * alpha_m, tau_max * alpha_m)\n",
    "\n",
    "    # Update C and Q\n",
    "    Q_next = nu * Q + 1\n",
    "    C = (nu * Q * (C + eta) + fp) / Q_next\n",
    "    Q = Q_next\n",
    "\n",
    "    return alpha, xp, fp, Fp, C, Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
