{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of the conjugate gradient algorithm from the scipy package, applied to complex-valued variables.\n",
    "# Not working for the ptychography case right now.\n",
    "\n",
    "To adapt the conjugate gradient algorithm, I changed the line search algorithm in linesearch.py to that described in the paper (Equation 3.10):\n",
    "\n",
    "https://epubs.siam.org/doi/10.1137/110832124\n",
    "\n",
    "Additionally, I also changed the return value of the `descent_condition` sub-function in `_minimize_cg` function from  \n",
    "`numpy.dot(pk, gfk) <= -sigma_3 * numpy.dot(gfk, gfk)`  \n",
    "to  \n",
    "`np.real(numpy.dot(np.conj(pk), gfk)) <= -sigma_3 * np.real(numpy.dot(np.conj(gfk), gfk))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "\n",
    "# Minimization routines\n",
    "\n",
    "__all__ = ['fmin', 'fmin_powell', 'fmin_bfgs', 'fmin_ncg', 'fmin_cg',\n",
    "           'fminbound', 'brent', 'golden', 'bracket', 'rosen', 'rosen_der',\n",
    "           'rosen_hess', 'rosen_hess_prod', 'brute', 'approx_fprime',\n",
    "           'line_search', 'check_grad', 'OptimizeResult', 'show_options',\n",
    "           'OptimizeWarning']\n",
    "\n",
    "__docformat__ = \"restructuredtext en\"\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import numpy\n",
    "from scipy._lib.six import callable, xrange\n",
    "from numpy import (atleast_1d, eye, mgrid, argmin, zeros, shape, squeeze,\n",
    "                   vectorize, asarray, sqrt, Inf, asfarray, isinf)\n",
    "import numpy as np\n",
    "from linesearch import (line_search_wolfe2,\n",
    "                        line_search_wolfe2 as line_search,\n",
    "                        LineSearchWarning)\n",
    "from scipy._lib._util import getargspec_no_self as _getargspec\n",
    "\n",
    "\n",
    "# standard status messages of optimizers\n",
    "_status_message = {'success': 'Optimization terminated successfully.',\n",
    "                   'maxfev': 'Maximum number of function evaluations has '\n",
    "                              'been exceeded.',\n",
    "                   'maxiter': 'Maximum number of iterations has been '\n",
    "                              'exceeded.',\n",
    "                   'pr_loss': 'Desired error not necessarily achieved due '\n",
    "                              'to precision loss.'}\n",
    "\n",
    "\n",
    "class MemoizeJac(object):\n",
    "    \"\"\" Decorator that caches the value gradient of function each time it\n",
    "    is called. \"\"\"\n",
    "    def __init__(self, fun):\n",
    "        self.fun = fun\n",
    "        self.jac = None\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, x, *args):\n",
    "        self.x = numpy.asarray(x).copy()\n",
    "        fg = self.fun(x, *args)\n",
    "        self.jac = fg[1]\n",
    "        return fg[0]\n",
    "\n",
    "    def derivative(self, x, *args):\n",
    "        if self.jac is not None and numpy.alltrue(x == self.x):\n",
    "            return self.jac\n",
    "        else:\n",
    "            self(x, *args)\n",
    "            return self.jac\n",
    "\n",
    "\n",
    "class OptimizeResult(dict):\n",
    "    \"\"\" Represents the optimization result.\n",
    "    Attributes\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The solution of the optimization.\n",
    "    success : bool\n",
    "        Whether or not the optimizer exited successfully.\n",
    "    status : int\n",
    "        Termination status of the optimizer. Its value depends on the\n",
    "        underlying solver. Refer to `message` for details.\n",
    "    message : str\n",
    "        Description of the cause of the termination.\n",
    "    fun, jac, hess: ndarray\n",
    "        Values of objective function, its Jacobian and its Hessian (if\n",
    "        available). The Hessians may be approximations, see the documentation\n",
    "        of the function in question.\n",
    "    hess_inv : object\n",
    "        Inverse of the objective function's Hessian; may be an approximation.\n",
    "        Not available for all solvers. The type of this attribute may be\n",
    "        either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
    "    nfev, njev, nhev : int\n",
    "        Number of evaluations of the objective functions and of its\n",
    "        Jacobian and Hessian.\n",
    "    nit : int\n",
    "        Number of iterations performed by the optimizer.\n",
    "    maxcv : float\n",
    "        The maximum constraint violation.\n",
    "    Notes\n",
    "    -----\n",
    "    There may be additional attributes not listed above depending of the\n",
    "    specific solver. Since this class is essentially a subclass of dict\n",
    "    with attribute accessors, one can see which attributes are available\n",
    "    using the `keys()` method.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.keys():\n",
    "            m = max(map(len, list(self.keys()))) + 1\n",
    "            return '\\n'.join([k.rjust(m) + ': ' + repr(v)\n",
    "                              for k, v in sorted(self.items())])\n",
    "        else:\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "    def __dir__(self):\n",
    "        return list(self.keys())\n",
    "\n",
    "\n",
    "class OptimizeWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _check_unknown_options(unknown_options):\n",
    "    if unknown_options:\n",
    "        msg = \", \".join(map(str, unknown_options.keys()))\n",
    "        # Stack level 4: this is called from _minimize_*, which is\n",
    "        # called from another function in Scipy. Level 4 is the first\n",
    "        # level in user code.\n",
    "        warnings.warn(\"Unknown solver options: %s\" % msg, OptimizeWarning, 4)\n",
    "\n",
    "\n",
    "def is_array_scalar(x):\n",
    "    \"\"\"Test whether `x` is either a scalar or an array scalar.\n",
    "    \"\"\"\n",
    "    return np.size(x) == 1\n",
    "\n",
    "\n",
    "_epsilon = sqrt(numpy.finfo(float).eps)\n",
    "\n",
    "\n",
    "def vecnorm(x, ord=2):\n",
    "    if ord == Inf:\n",
    "        return numpy.amax(numpy.abs(x))\n",
    "    elif ord == -Inf:\n",
    "        return numpy.amin(numpy.abs(x))\n",
    "    else:\n",
    "        return numpy.sum(numpy.abs(x)**ord, axis=0)**(1.0 / ord)\n",
    "\n",
    "def wrap_function(function, args):\n",
    "    ncalls = [0]\n",
    "    if function is None:\n",
    "        return ncalls, None\n",
    "\n",
    "    def function_wrapper(*wrapper_args):\n",
    "        ncalls[0] += 1\n",
    "        return function(*(wrapper_args + args))\n",
    "\n",
    "    return ncalls, function_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LineSearchError(RuntimeError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval,\n",
    "                         **kwargs):\n",
    "    \"\"\"\n",
    "    Same as line_search_wolfe1, but fall back to line_search_wolfe2 if\n",
    "    suitable step length is not found, and raise an exception if a\n",
    "    suitable step length is not found.\n",
    "    Raises\n",
    "    ------\n",
    "    _LineSearchError\n",
    "        If no suitable step size is found\n",
    "    \"\"\"\n",
    "\n",
    "    extra_condition = kwargs.pop('extra_condition', None)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', LineSearchWarning)\n",
    "        kwargs2 = {}\n",
    "        for key in ('c1', 'c2', 'amax'):\n",
    "            if key in kwargs:\n",
    "                kwargs2[key] = kwargs[key]\n",
    "        ret = line_search_wolfe2(f, fprime, xk, pk, gfk,\n",
    "                                 old_fval, old_old_fval,\n",
    "                                 extra_condition=extra_condition,\n",
    "                                 **kwargs2)\n",
    "\n",
    "    if ret[0] is None:\n",
    "        raise _LineSearchError()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmin_cg(f, x0, fprime=None, args=(), gtol=1e-5, norm=Inf, epsilon=_epsilon,\n",
    "            maxiter=None, full_output=0, disp=1, retall=0, callback=None):\n",
    "    \"\"\"\n",
    "    Minimize a function using a nonlinear conjugate gradient algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable, ``f(x, *args)``\n",
    "        Objective function to be minimized.  Here `x` must be a 1-D array of\n",
    "        the variables that are to be changed in the search for a minimum, and\n",
    "        `args` are the other (fixed) parameters of `f`.\n",
    "    x0 : ndarray\n",
    "        A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
    "        It must be a 1-D array of values.\n",
    "    fprime : callable, ``fprime(x, *args)``, optional\n",
    "        A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
    "        are as described above for `f`. The returned value must be a 1-D array.\n",
    "        Defaults to None, in which case the gradient is approximated\n",
    "        numerically (see `epsilon`, below).\n",
    "    args : tuple, optional\n",
    "        Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
    "        additional fixed parameters are needed to completely specify the\n",
    "        functions `f` and `fprime`.\n",
    "    gtol : float, optional\n",
    "        Stop when the norm of the gradient is less than `gtol`.\n",
    "    norm : float, optional\n",
    "        Order to use for the norm of the gradient\n",
    "        (``-np.Inf`` is min, ``np.Inf`` is max).\n",
    "    epsilon : float or ndarray, optional\n",
    "        Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
    "        scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
    "        floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
    "        1.5e-8.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
    "    full_output : bool, optional\n",
    "        If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
    "        addition to `xopt`.  See the Returns section below for additional\n",
    "        information on optional return values.\n",
    "    disp : bool, optional\n",
    "        If True, return a convergence message, followed by `xopt`.\n",
    "    retall : bool, optional\n",
    "        If True, add to the returned values the results of each iteration.\n",
    "    callback : callable, optional\n",
    "        An optional user-supplied function, called after each iteration.\n",
    "        Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
    "    Returns\n",
    "    -------\n",
    "    xopt : ndarray\n",
    "        Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
    "    fopt : float, optional\n",
    "        Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
    "    func_calls : int, optional\n",
    "        The number of function_calls made.  Only returned if `full_output`\n",
    "        is True.\n",
    "    grad_calls : int, optional\n",
    "        The number of gradient calls made. Only returned if `full_output` is\n",
    "        True.\n",
    "    warnflag : int, optional\n",
    "        Integer value with warning status, only returned if `full_output` is\n",
    "        True.\n",
    "        0 : Success.\n",
    "        1 : The maximum number of iterations was exceeded.\n",
    "        2 : Gradient and/or function calls were not changing.  May indicate\n",
    "            that precision was lost, i.e., the routine did not converge.\n",
    "    allvecs : list of ndarray, optional\n",
    "        List of arrays, containing the results at each iteration.\n",
    "        Only returned if `retall` is True.\n",
    "    See Also\n",
    "    --------\n",
    "    minimize : common interface to all `scipy.optimize` algorithms for\n",
    "               unconstrained and constrained minimization of multivariate\n",
    "               functions.  It provides an alternative way to call\n",
    "               ``fmin_cg``, by specifying ``method='CG'``.\n",
    "    Notes\n",
    "    -----\n",
    "    This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
    "    [1]_.\n",
    "    Conjugate gradient methods tend to work better when:\n",
    "    1. `f` has a unique global minimizing point, and no local minima or\n",
    "       other stationary points,\n",
    "    2. `f` is, at least locally, reasonably well approximated by a\n",
    "       quadratic function of the variables,\n",
    "    3. `f` is continuous and has a continuous gradient,\n",
    "    4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
    "    5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
    "       minimizing point, `xopt`.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
    "    Examples\n",
    "    --------\n",
    "    Example 1: seek the minimum value of the expression\n",
    "    ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
    "    of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
    "    >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
    "    >>> def f(x, *args):\n",
    "    ...     u, v = x\n",
    "    ...     a, b, c, d, e, f = args\n",
    "    ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
    "    >>> def gradf(x, *args):\n",
    "    ...     u, v = x\n",
    "    ...     a, b, c, d, e, f = args\n",
    "    ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
    "    ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
    "    ...     return np.asarray((gu, gv))\n",
    "    >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
    "    >>> from scipy import optimize\n",
    "    >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
    "    Optimization terminated successfully.\n",
    "             Current function value: 1.617021\n",
    "             Iterations: 4\n",
    "             Function evaluations: 8\n",
    "             Gradient evaluations: 8\n",
    "    >>> res1\n",
    "    array([-1.80851064, -0.25531915])\n",
    "    Example 2: solve the same problem using the `minimize` function.\n",
    "    (This `myopts` dictionary shows all of the available options,\n",
    "    although in practice only non-default values would be needed.\n",
    "    The returned value will be a dictionary.)\n",
    "    >>> opts = {'maxiter' : None,    # default value.\n",
    "    ...         'disp' : True,    # non-default value.\n",
    "    ...         'gtol' : 1e-5,    # default value.\n",
    "    ...         'norm' : np.inf,  # default value.\n",
    "    ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
    "    >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
    "    ...                          method='CG', options=opts)\n",
    "    Optimization terminated successfully.\n",
    "            Current function value: 1.617021\n",
    "            Iterations: 4\n",
    "            Function evaluations: 8\n",
    "            Gradient evaluations: 8\n",
    "    >>> res2.x  # minimum found\n",
    "    array([-1.80851064, -0.25531915])\n",
    "    \"\"\"\n",
    "    opts = {'gtol': gtol,\n",
    "            'norm': norm,\n",
    "            'eps': epsilon,\n",
    "            'disp': disp,\n",
    "            'maxiter': maxiter,\n",
    "            'return_all': retall}\n",
    "\n",
    "    res = _minimize_cg(f, x0, args, fprime, callback=callback, **opts)\n",
    "\n",
    "    if full_output:\n",
    "        retlist = res['x'], res['fun'], res['nfev'], res['njev'], res['status']\n",
    "        if retall:\n",
    "            retlist += (res['allvecs'], )\n",
    "        return retlist\n",
    "    else:\n",
    "        if retall:\n",
    "            return res['x'], res['allvecs']\n",
    "        else:\n",
    "            return res['x']\n",
    "\n",
    "\n",
    "def _minimize_cg(fun, x0, args=(), jac=None, callback=None,\n",
    "                 gtol=1e-5, norm=Inf, eps=_epsilon, maxiter=None,\n",
    "                 disp=False, return_all=False,\n",
    "                 **unknown_options):\n",
    "    \"\"\"\n",
    "    Minimization of scalar function of one or more variables using the\n",
    "    conjugate gradient algorithm.\n",
    "    Options\n",
    "    -------\n",
    "    disp : bool\n",
    "        Set to True to print convergence messages.\n",
    "    maxiter : int\n",
    "        Maximum number of iterations to perform.\n",
    "    gtol : float\n",
    "        Gradient norm must be less than `gtol` before successful\n",
    "        termination.\n",
    "    norm : float\n",
    "        Order of norm (Inf is max, -Inf is min).\n",
    "    eps : float or ndarray\n",
    "        If `jac` is approximated, use this value for the step size.\n",
    "    \"\"\"\n",
    "    _check_unknown_options(unknown_options)\n",
    "    f = fun\n",
    "    fprime = jac\n",
    "    epsilon = eps\n",
    "    retall = return_all\n",
    "\n",
    "    x0 = asarray(x0).flatten()\n",
    "    if maxiter is None:\n",
    "        maxiter = len(x0) * 200\n",
    "    func_calls, f = wrap_function(f, args)\n",
    "    if fprime is None:\n",
    "        grad_calls, myfprime = wrap_function(approx_fprime, (f, epsilon))\n",
    "    else:\n",
    "        grad_calls, myfprime = wrap_function(fprime, args)\n",
    "    gfk = myfprime(x0)\n",
    "    k = 0\n",
    "    xk = x0\n",
    "\n",
    "    # Sets the initial step guess to dx ~ 1\n",
    "    old_fval = f(xk)\n",
    "    old_old_fval = old_fval + np.linalg.norm(gfk) / 2\n",
    "\n",
    "    if retall:\n",
    "        allvecs = [xk]\n",
    "    warnflag = 0\n",
    "    pk = -gfk\n",
    "    gnorm = vecnorm(gfk, ord=norm)\n",
    "\n",
    "    sigma_3 = 0.01\n",
    "\n",
    "    while (gnorm > gtol) and (k < maxiter):\n",
    "        deltak = numpy.dot(gfk, gfk)\n",
    "\n",
    "        cached_step = [None]\n",
    "\n",
    "        def polak_ribiere_powell_step(alpha, gfkp1=None):\n",
    "            xkp1 = xk + alpha * pk\n",
    "            if gfkp1 is None:\n",
    "                gfkp1 = myfprime(xkp1)\n",
    "            yk = gfkp1 - gfk\n",
    "            beta_k = max(0, numpy.dot(yk, gfkp1) / deltak)\n",
    "            pkp1 = -gfkp1 + beta_k * pk\n",
    "            gnorm = vecnorm(gfkp1, ord=norm)\n",
    "            return (alpha, xkp1, pkp1, gfkp1, gnorm)\n",
    "\n",
    "        def descent_condition(alpha, xkp1, fp1, gfkp1):\n",
    "            # Polak-Ribiere+ needs an explicit check of a sufficient\n",
    "            # descent condition, which is not guaranteed by strong Wolfe.\n",
    "            #\n",
    "            # See Gilbert & Nocedal, \"Global convergence properties of\n",
    "            # conjugate gradient methods for optimization\",\n",
    "            # SIAM J. Optimization 2, 21 (1992).\n",
    "            cached_step[:] = polak_ribiere_powell_step(alpha, gfkp1)\n",
    "            alpha, xk, pk, gfk, gnorm = cached_step\n",
    "\n",
    "            # Accept step if it leads to convergence.\n",
    "            if gnorm <= gtol:\n",
    "                return True\n",
    "\n",
    "            # Accept step if sufficient descent condition applies.\n",
    "            return np.real(numpy.dot(np.conj(pk), gfk)) <= -sigma_3 * np.real(numpy.dot(np.conj(gfk), gfk))\n",
    "\n",
    "        try:\n",
    "            alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n",
    "                     _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n",
    "                                          old_old_fval, c2=0.4, amin=1e-100, amax=1e100,\n",
    "                                          extra_condition=descent_condition)\n",
    "        except _LineSearchError:\n",
    "            # Line search failed to find a better solution.\n",
    "            warnflag = 2\n",
    "            break\n",
    "\n",
    "        # Reuse already computed results if possible\n",
    "        if alpha_k == cached_step[0]:\n",
    "            alpha_k, xk, pk, gfk, gnorm = cached_step\n",
    "        else:\n",
    "            alpha_k, xk, pk, gfk, gnorm = polak_ribiere_powell_step(alpha_k, gfkp1)\n",
    "\n",
    "        if retall:\n",
    "            allvecs.append(xk)\n",
    "        if callback is not None:\n",
    "            callback(xk)\n",
    "        k += 1\n",
    "        \n",
    "\n",
    "    fval = old_fval\n",
    "    if warnflag == 2:\n",
    "        msg = _status_message['pr_loss']\n",
    "    elif k >= maxiter:\n",
    "        warnflag = 1\n",
    "        msg = _status_message['maxiter']\n",
    "    else:\n",
    "        msg = _status_message['success']\n",
    "\n",
    "    if disp:\n",
    "        print(\"%s%s\" % (\"Warning: \" if warnflag != 0 else \"\", msg))\n",
    "        print(\"         Current function value: %f\" % fval)\n",
    "        print(\"         Iterations: %d\" % k)\n",
    "        print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "        print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "\n",
    "    result = OptimizeResult(fun=fval, jac=gfk, nfev=func_calls[0],\n",
    "                            njev=grad_calls[0], status=warnflag,\n",
    "                            success=(warnflag == 0), message=msg, x=xk,\n",
    "                            nit=k)\n",
    "    if retall:\n",
    "        result['allvecs'] = allvecs\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
